# ðŸ§  MetodologÃ­a de Machine Learning

Este documento explica en detalle la metodologÃ­a utilizada para construir, entrenar y evaluar el modelo LSTM de predicciÃ³n de acciones. Se basa en el notebook `Resources_tf/stock_prediction_using_LSTM.ipynb`.

---

## ðŸ“‹ Ãndice

1. [VisiÃ³n General](#1-visiÃ³n-general)
2. [RecolecciÃ³n de Datos](#2-recolecciÃ³n-de-datos)
3. [ExploraciÃ³n y VisualizaciÃ³n](#3-exploraciÃ³n-y-visualizaciÃ³n)
4. [IngenierÃ­a de Features](#4-ingenierÃ­a-de-features)
5. [Preprocesamiento](#5-preprocesamiento)
6. [CreaciÃ³n de Secuencias](#6-creaciÃ³n-de-secuencias)
7. [Arquitectura del Modelo](#7-arquitectura-del-modelo)
8. [Entrenamiento](#8-entrenamiento)
9. [EvaluaciÃ³n](#9-evaluaciÃ³n)
10. [PredicciÃ³n en ProducciÃ³n](#10-predicciÃ³n-en-producciÃ³n)

---

## 1. VisiÃ³n General

### Â¿Por quÃ© LSTM?

Las redes **LSTM (Long Short-Term Memory)** son un tipo especial de red neuronal recurrente (RNN) diseÃ±adas para aprender dependencias a largo plazo en secuencias de datos.

**Ventajas para series temporales financieras:**
- Capturan patrones temporales complejos
- Mantienen memoria de eventos pasados relevantes
- Manejan el problema del "vanishing gradient" de las RNN tradicionales

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Arquitectura LSTM                                 â”‚
â”‚                                                                          â”‚
â”‚   Input (t-100) â”€â”€â–¶ LSTM â”€â”€â–¶ LSTM â”€â”€â–¶ ... â”€â”€â–¶ LSTM â”€â”€â–¶ Output (t)      â”‚
â”‚                      â”‚        â”‚                 â”‚                        â”‚
â”‚                      â–¼        â–¼                 â–¼                        â”‚
â”‚                   Hidden   Hidden            Hidden                      â”‚
â”‚                    State    State             State                      â”‚
â”‚                                                                          â”‚
â”‚   "El modelo aprende quÃ© informaciÃ³n del pasado es relevante            â”‚
â”‚    para predecir el precio futuro"                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. RecolecciÃ³n de Datos

### Fuente de Datos: Yahoo Finance

Utilizamos la librerÃ­a `yfinance` para obtener datos histÃ³ricos del mercado:

```python
import yfinance as yf
from datetime import datetime

# Configurar rango de 10 aÃ±os
now = datetime.now()
start = datetime(now.year - 10, now.month, now.day)
end = now

# Descargar datos del ticker
ticker = 'AAPL'
df = yf.download(ticker, start, end)
```

### Estructura de los Datos

| Columna | DescripciÃ³n |
|---------|-------------|
| Date | Fecha del trading (Ã­ndice) |
| Open | Precio de apertura |
| High | Precio mÃ¡s alto del dÃ­a |
| Low | Precio mÃ¡s bajo del dÃ­a |
| **Close** | Precio de cierre (usado para predicciÃ³n) |
| Adj Close | Precio ajustado por dividendos/splits |
| Volume | Volumen de transacciones |

> ðŸ“Œ **Nota**: Usamos el precio de **Close** (cierre) como variable objetivo por ser el mÃ¡s representativo del valor de la acciÃ³n al final del dÃ­a.

### ValidaciÃ³n de Datos

```python
# Verificar datos faltantes
df.isna().sum()

# EstadÃ­sticas descriptivas
df.describe()

# Tipos de datos
df.dtypes
```

---

## 3. ExploraciÃ³n y VisualizaciÃ³n

### VisualizaciÃ³n del Precio de Cierre

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.plot(df.Close)
plt.title(f'{ticker} - Precio de Cierre')
plt.xlabel('DÃ­as')
plt.ylabel('Precio ($)')
plt.show()
```

**Resultado tÃ­pico (AAPL):**

```
Precio ($)
â”‚
250 â”‚                                          â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    â”‚                                     â•­â”€â”€â”€â”€â•¯           â”‚
200 â”‚                               â•­â”€â”€â”€â”€â”€â•¯                â•°â”€â”€
    â”‚                          â•­â”€â”€â”€â”€â•¯
150 â”‚                     â•­â”€â”€â”€â”€â•¯
    â”‚               â•­â”€â”€â”€â”€â”€â•¯
100 â”‚          â•­â”€â”€â”€â”€â•¯
    â”‚     â•­â”€â”€â”€â”€â•¯
 50 â”‚â”€â”€â”€â”€â”€â•¯
    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ DÃ­as
          2015      2017      2019      2021      2023      2025
```

---

## 4. IngenierÃ­a de Features

### Medias MÃ³viles (Moving Averages)

Las medias mÃ³viles suavizan las fluctuaciones de precio y revelan tendencias:

```python
# Media mÃ³vil de 100 dÃ­as
df['MA_100'] = df.Close.rolling(100).mean()

# Media mÃ³vil de 200 dÃ­as
df['MA_200'] = df.Close.rolling(200).mean()
```

**InterpretaciÃ³n:**
- **MA corta (100) > MA larga (200)**: Tendencia alcista (bullish)
- **MA corta (100) < MA larga (200)**: Tendencia bajista (bearish)
- **Cruce de MAs**: SeÃ±al de posible cambio de tendencia

### CÃ¡lculo de Cambio Porcentual

```python
# Porcentaje de cambio diario
df['Percentage Changed'] = df.Close.pct_change()
```

**Â¿Por quÃ© es Ãºtil?**
- Normaliza los cambios independientemente del precio absoluto
- Muestra volatilidad del mercado
- Permite comparar diferentes acciones

---

## 5. Preprocesamiento

### DivisiÃ³n Train/Test

```python
# 70% para entrenamiento, 30% para testing
data_training = pd.DataFrame(df.Close[0:int(len(df)*0.7)])
data_testing = pd.DataFrame(df.Close[int(len(df)*0.7):])
```

**Ejemplo con 2500 dÃ­as de datos:**
- Training: 1750 dÃ­as (70%)
- Testing: 750 dÃ­as (30%)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Dataset Completo                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         TRAINING (70%)           â”‚           TESTING (30%)          â”‚
â”‚      DÃ­as 0 - 1749               â”‚        DÃ­as 1750 - 2499          â”‚
â”‚                                  â”‚                                  â”‚
â”‚  âœ“ Usado para entrenar modelo    â”‚  âœ“ Usado para evaluar modelo    â”‚
â”‚  âœ“ Scaler fitted AQUÃ            â”‚  âœ— Scaler NO fitted aquÃ­        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### NormalizaciÃ³n (Scaling)

Los precios pueden variar de $10 a $250, pero las redes neuronales funcionan mejor con valores entre 0 y 1:

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1))
data_training_array = scaler.fit_transform(data_training)
```

**FÃ³rmula MinMaxScaler:**

$$X_{scaled} = \frac{X - X_{min}}{X_{max} - X_{min}}$$

**Ejemplo:**
- Precio mÃ­nimo en training: $27.00
- Precio mÃ¡ximo en training: $183.00
- Precio actual: $105.00

$$X_{scaled} = \frac{105 - 27}{183 - 27} = \frac{78}{156} = 0.5$$

### âš ï¸ CRÃTICO: PrevenciÃ³n de Data Leakage

```python
# âŒ INCORRECTO - Data Leakage
scaler.fit_transform(df.Close)  # Incluye datos de testing!

# âœ… CORRECTO - Sin Data Leakage
scaler.fit(data_training)  # Solo datos de training
scaler.transform(data_testing)  # Aplicar a testing
```

**Â¿Por quÃ© es importante?**
- El scaler "conoce" los valores mÃ­nimo y mÃ¡ximo
- Si incluye datos de testing, el modelo "ve el futuro"
- Las mÃ©tricas serÃ­an artificialmente buenas pero irreales

---

## 6. CreaciÃ³n de Secuencias

### El Concepto

LSTM necesita secuencias de tiempo como entrada. Usamos los **Ãºltimos 100 dÃ­as** para predecir el **dÃ­a siguiente**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CreaciÃ³n de Secuencias (Window Sliding)               â”‚
â”‚                                                                          â”‚
â”‚  Datos: [Pâ‚€, Pâ‚, Pâ‚‚, Pâ‚ƒ, Pâ‚„, Pâ‚…, Pâ‚†, ..., Pâ‚â‚€â‚€, Pâ‚â‚€â‚, Pâ‚â‚€â‚‚, ...]       â”‚
â”‚                                                                          â”‚
â”‚  Secuencia 1:  x = [Pâ‚€  ... Pâ‚‰â‚‰]   â†’  y = Pâ‚â‚€â‚€                          â”‚
â”‚  Secuencia 2:  x = [Pâ‚  ... Pâ‚â‚€â‚€]  â†’  y = Pâ‚â‚€â‚                          â”‚
â”‚  Secuencia 3:  x = [Pâ‚‚  ... Pâ‚â‚€â‚]  â†’  y = Pâ‚â‚€â‚‚                          â”‚
â”‚  ...                                                                     â”‚
â”‚                                                                          â”‚
â”‚  El modelo aprende: "Dados 100 dÃ­as, Â¿cuÃ¡l es el precio del dÃ­a 101?"   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplementaciÃ³n

```python
x_train = []
y_train = []

for i in range(100, data_training_array.shape[0]):
    x_train.append(data_training_array[i-100:i])  # 100 precios anteriores
    y_train.append(data_training_array[i, 0])     # Precio actual (target)

x_train, y_train = np.array(x_train), np.array(y_train)
```

### Ejemplo Simplificado

```python
# Supongamos precios: [10, 20, 30, 40, 50, 60]
# Con ventana de 3 dÃ­as:

precios = [10, 20, 30, 40, 50, 60]

# Secuencia 1: x = [10, 20, 30], y = 40
# Secuencia 2: x = [20, 30, 40], y = 50
# Secuencia 3: x = [30, 40, 50], y = 60
```

### Dimensiones de los Datos

```python
x_train.shape  # (1650, 100, 1) = (muestras, timesteps, features)
y_train.shape  # (1650,) = (muestras,)
```

- **Muestras**: ~1650 secuencias de entrenamiento
- **Timesteps**: 100 dÃ­as por secuencia
- **Features**: 1 (solo precio de cierre)

---

## 7. Arquitectura del Modelo

### DiseÃ±o de la Red LSTM

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Input

model = Sequential()

# Capa de entrada
model.add(Input(shape=(100, 1)))

# Primera capa LSTM
model.add(LSTM(units=128, activation='tanh', return_sequences=True))

# Segunda capa LSTM
model.add(LSTM(units=64))

# Capa oculta densa
model.add(Dense(25))

# Capa de salida
model.add(Dense(1))
```

### VisualizaciÃ³n de la Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Arquitectura del Modelo                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      Input Layer                                 â”‚    â”‚
â”‚  â”‚                   Shape: (100, 1)                                â”‚    â”‚
â”‚  â”‚           100 timesteps Ã— 1 feature (precio)                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                     LSTM Layer 1                                 â”‚    â”‚
â”‚  â”‚                    128 unidades                                  â”‚    â”‚
â”‚  â”‚               activation: tanh                                   â”‚    â”‚
â”‚  â”‚            return_sequences: True                                â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  "Procesa secuencia y pasa estados ocultos a la siguiente capa" â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                     LSTM Layer 2                                 â”‚    â”‚
â”‚  â”‚                     64 unidades                                  â”‚    â”‚
â”‚  â”‚               activation: tanh                                   â”‚    â”‚
â”‚  â”‚            return_sequences: False                               â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  "Extrae representaciÃ³n final de la secuencia"                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                     Dense Layer                                  â”‚    â”‚
â”‚  â”‚                    25 neuronas                                   â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  "Transforma features LSTM en representaciÃ³n mÃ¡s compacta"       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                     Output Layer                                 â”‚    â”‚
â”‚  â”‚                     1 neurona                                    â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  "Predice el precio del dÃ­a siguiente (normalizado 0-1)"         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Total de parÃ¡metros: ~120,000                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Â¿Por quÃ© estos hiperparÃ¡metros?

| HiperparÃ¡metro | Valor | JustificaciÃ³n |
|----------------|-------|---------------|
| Secuencia | 100 dÃ­as | ~5 meses de trading, captura tendencias medias |
| LSTM 1 | 128 unidades | Capacidad suficiente para patrones complejos |
| LSTM 2 | 64 unidades | ReducciÃ³n dimensional, evita overfitting |
| ActivaciÃ³n | tanh | EstÃ¡ndar para LSTM, valores (-1, 1) |
| return_sequences | True/False | True para pasar a otra LSTM, False para Dense |

---

## 8. Entrenamiento

### ConfiguraciÃ³n del Entrenamiento

```python
model.compile(
    optimizer='adam',
    loss='mean_squared_error'
)

model.fit(
    x_train, 
    y_train, 
    epochs=50
)
```

### Detalles del Proceso

| ParÃ¡metro | Valor | DescripciÃ³n |
|-----------|-------|-------------|
| Optimizer | Adam | Adaptive learning rate, rÃ¡pido convergencia |
| Loss | MSE | Mean Squared Error, penaliza errores grandes |
| Epochs | 50 | Iteraciones completas sobre el dataset |
| Batch Size | 32 (default) | Muestras por actualizaciÃ³n de pesos |

### Curva de PÃ©rdida TÃ­pica

```
Loss
â”‚
0.05 â”‚â–ˆ
     â”‚ â–ˆ
0.04 â”‚  â–ˆ
     â”‚   â–ˆ
0.03 â”‚    â–ˆ
     â”‚     â–ˆ
0.02 â”‚      â–ˆ
     â”‚       â–ˆâ–ˆ
0.01 â”‚         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     â”‚
0.00 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Epochs
     0    10    20    30    40    50
```

### Guardar el Modelo

```python
model.save('stock_prediction_model.keras')
```

---

## 9. EvaluaciÃ³n

### PreparaciÃ³n de Datos de Test

```python
# Tomar Ãºltimos 100 dÃ­as del training para iniciar secuencias de test
past_100_days = data_training.tail(100)

# Concatenar con datos de test
final_df = pd.concat([past_100_days, data_testing], ignore_index=True)

# Escalar usando el scaler de training (Â¡NO fit_transform!)
input_data = scaler.transform(final_df)

# Crear secuencias de test
x_test, y_test = [], []
for i in range(100, input_data.shape[0]):
    x_test.append(input_data[i-100:i])
    y_test.append(input_data[i, 0])

x_test, y_test = np.array(x_test), np.array(y_test)
```

### PredicciÃ³n y DesnormalizaciÃ³n

```python
# Predecir
y_predicted = model.predict(x_test)

# Volver a escala original
y_predicted = scaler.inverse_transform(y_predicted).flatten()
y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
```

### MÃ©tricas de EvaluaciÃ³n

#### 1. Mean Squared Error (MSE)

$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

```python
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_predicted)
```

**InterpretaciÃ³n:**
- Mide el promedio de los errores al cuadrado
- Penaliza errores grandes mÃ¡s que pequeÃ±os
- En la misma escala que el precio al cuadrado

#### 2. Root Mean Squared Error (RMSE)

$$RMSE = \sqrt{MSE}$$

```python
rmse = np.sqrt(mse)
```

**InterpretaciÃ³n:**
- En la misma escala que el precio ($)
- RMSE = 3.5 significa error promedio de ~$3.50

#### 3. R-Squared (RÂ²)

$$R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}$$

```python
from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_predicted)
```

**InterpretaciÃ³n:**
- Porcentaje de varianza explicada por el modelo
- RÂ² = 0.98 significa que el modelo explica 98% de la variabilidad
- Valores cercanos a 1.0 son excelentes

### Resultados TÃ­picos

| MÃ©trica | Valor | EvaluaciÃ³n |
|---------|-------|------------|
| MSE | ~12-15 | - |
| RMSE | ~$3.5-4.0 | Excelente para predicciÃ³n diaria |
| RÂ² | ~0.97-0.99 | Muy alto, buen ajuste |

### VisualizaciÃ³n de Resultados

```python
plt.figure(figsize=(12, 6))
plt.plot(y_test, 'b', label='Precio Real')
plt.plot(y_predicted, 'r', label='Precio Predicho')
plt.xlabel('DÃ­as')
plt.ylabel('Precio ($)')
plt.legend()
```

```
Precio ($)
â”‚
200 â”‚   â•­â”€â”€â”€â”€â”€â•®                    â•­â”€â”€â”€â”€â”€â”€
    â”‚  â•±      â•²                  â•­â•¯
180 â”‚ â•±        â•²      â•­â”€â”€â”€â”€â”€â•®   â•±
    â”‚â•±          â•²    â•±      â•²  â•±       â”€â”€ Real
160 â”‚            â•²â”€â”€â•¯        â•²â•±        â”€â”€ Predicho
    â”‚
140 â”‚
    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ DÃ­as Test
            200       400       600
```

---

## 10. PredicciÃ³n en ProducciÃ³n

### Backtesting vs PredicciÃ³n Futura

| Modo | DescripciÃ³n | Uso |
|------|-------------|-----|
| **Backtesting** | EvalÃºa modelo con datos histÃ³ricos reservados | Validar precisiÃ³n |
| **PredicciÃ³n Futura** | Predice dÃ­as que aÃºn no han ocurrido | PronÃ³stico real |

### PredicciÃ³n Recursiva

Para predecir mÃºltiples dÃ­as en el futuro, usamos predicciÃ³n recursiva:

```
DÃ­a 0: [Pâ‚, Pâ‚‚, ..., Pâ‚â‚€â‚€] â†’ Predice PÌ‚â‚â‚€â‚
DÃ­a 1: [Pâ‚‚, Pâ‚ƒ, ..., PÌ‚â‚â‚€â‚] â†’ Predice PÌ‚â‚â‚€â‚‚
DÃ­a 2: [Pâ‚ƒ, Pâ‚„, ..., PÌ‚â‚â‚€â‚‚] â†’ Predice PÌ‚â‚â‚€â‚ƒ
...
```

### CuantificaciÃ³n de Incertidumbre

Usamos **Monte Carlo Dropout** para estimar la incertidumbre:

```python
mc_iterations = 50
predictions = []

for _ in range(mc_iterations):
    pred = model(X, training=True)  # training=True activa dropout
    predictions.append(pred)

mean_pred = np.mean(predictions)
std_pred = np.std(predictions)

# Intervalo de confianza 95%
lower = mean_pred - 1.96 * std_pred
upper = mean_pred + 1.96 * std_pred
```

### Factor de Incertidumbre Creciente

```python
# La incertidumbre crece con el horizonte de predicciÃ³n
uncertainty_factor = 1.0 + (0.02 * day)  # +2% por dÃ­a
adjusted_std = std_pred * uncertainty_factor
```

**VisualizaciÃ³n:**

```
Precio ($)
â”‚
â”‚                              â•±â•²
â”‚                            â•±    â•²  â† Banda superior (95%)
â”‚                          â•±        â•²
â”‚                        â•±   â”€â”€â”€â”€â”€â”€â”€â”€  â† PredicciÃ³n media
â”‚                      â•±    â•²
â”‚                    â•±        â•²  â† Banda inferior (95%)
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±
â”‚     HistÃ³rico    â”‚  PredicciÃ³n Futura
â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ DÃ­as
                   Hoy
```

---

## ðŸ“š Referencias

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) - Chris Olah
- [Keras Documentation](https://keras.io/api/layers/recurrent_layers/lstm/)
- [yfinance Documentation](https://pypi.org/project/yfinance/)

---

## ðŸ”œ Siguiente Paso

Consulta la [DocumentaciÃ³n de la API](API.md) para ver cÃ³mo usar estas predicciones en la aplicaciÃ³n.
